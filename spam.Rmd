---
title: "Spam"
author: "Thadryan Sweeney"
date: "November 7, 2017"
output:
  pdf_document: default
  word_document: default
  html_document: default
---

```{r question 1 - get data}

# read and inspect the raw data
sms_raw <- read.csv('C:/Users/Thadryan.Hank-PC/Documents/R/da5030.spammsgdataset.csv', stringsAsFactors = TRUE)
str(sms_raw)
```

We'll make a table to get an idea of the data looks like in terms of our areas of interest, in this case, spam and ham. We see the the majority of the set is composed of what we're looking for, but the unwanted messages are common enough to be problematic. 

```{r table}

# create a table based on type 
table(sms_raw$type)
```

R has great library support in general, and text mining is no exception. We can import the tm library and use it to create a corpus of data

```{r import text mining packages}

# this is a library for text mining
library(tm)

# we make a body of text to mine. The "V" stands for volatile, meaning it is not store permanantly on the hardrive. 
sms_corpus <- VCorpus(VectorSource(sms_raw$text))

# display the traits
print(sms_corpus)

# inspec the first two 
inspect(sms_corpus[1:2])

# view the content of a message
as.character(sms_corpus[[1]])

# view more than one 
lapply(sms_corpus[1:2], as.character)

# start cleaning text
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
                           
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])

# update the data with removal of numbers, stopwords (to, but, and), and punctuation 
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
```

Next we will convert works into thier roots, for example "writing" becomes "write". This keeps the themes legitimate without cluttering the dataset with redundancy and making it difficult to count. 

```{r snowball}

# this library will allow use to convert forms of the words to roots
library(SnowballC)
```


Numerous steps still need to take place before the dataset is ready to use, however. We'll need to remove whitespace and create a matrix. There is also a function that could speed up this process for next time now that we understand it.

```{r clean}

sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)

# remove extra whitespace 
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)

# create a document term matrix from the data
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)

#demostrate function parameters that could speed up the whole process 
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(
  tolower = TRUE,
  removeNumbers = TRUE,
  stopwords = TRUE,
  removePunctuation = TRUE,
  stemming = TRUE))
sms_dtm
sms_dtm2
```


# Partition the data


Now we can create training and validation datasets, and create the labels for what we are trying to predict. We will also observe the proportions in the datasets, to ensure we haven't accidentally loaded the dice by putting a drastically different proportion in one than the other. 

```{r partition data}

# break into testing and training set
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]


# set labels while we are at it 
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type

# confirm we are on the right track 
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
```

# Visualize the data 

Now that we have that taken care of, we can proceede though some analytical steps and then build our model.
One thing we want to know about is word frequency. Wordclouds are a great way to get an idea of paterns intuitively.

```{r wordcloud}

# get the library 
library(wordcloud)

#call the function 
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
```

Let's compare that to the junk folder. If we're on the right track, there should be a difference, which we will later quantify and use for our classifier. 

```{r wordcloud spam}

# call the subsets
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type == "ham")

# pass to the wordcloud function 
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
```

Now let's get more quantitative about the frequency, finding some areas the keep popping up.

```{r frequency terms}

# call the frequency counter with argument for number of occurences
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)

# see if there is a difference in the sets
sms_dtm_freq_train <- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
```

We will now make a function to convert to yes/no values for a more reader friendly output.

```{r convert function}

# define function 
convert_counts <- function(x)
{
  x <- ifelse(x > 0, "yes", "no")
}

# apply the function by rows to the datasets
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
```

Next we will get a few more libraries. These will be used for the actual classification and to see how our model preforms.

```{r import more libraries}

# get required materials
library("e1071")
library("gmodels")

# call the naive bayes function
sms_classifier <- naiveBayes(sms_train, sms_train_labels)

# use it to make a predition
sms_test_pred <- predict(sms_classifier, sms_test)

# inspect the results 
CrossTable(sms_test_pred, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
```

Ont thing we haven't talked about so far in this context is the Laplace approximator. Naive Bayes classifiers work by multiplying probabiliteis derived from imperical values. This means that if we're looking at a term that didn't occur, we end up multiplying by zero and nullifying our results. To avoid having the function need to work around this, we can simply replace the value with a 1, which will likely cause very little, if any, perceptible disturbance in the functions.

```{r improving performance}

# add laplace argument, replacing zeros with ones 
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 1)

# make new predictor 
sms_test_pred2 <- predict(sms_classifier2, sms_test)

# visualize the results 
CrossTable(sms_test_pred2, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
```
