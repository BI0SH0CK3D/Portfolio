---
title: "Practicum 2"
author: "Thadryan Sweeney"
date: "October 29, 2017"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Question 1

Question one will see us analyze some census and assemble a Naive Bayes classifier from scratch. 

(10 pts) Create a frequency and then a likelihood table for the categorical features in the data set. Build your own Naive Bayes classifier for those features.

(30 pts)Predict the binomial class membership for a white male adult who is a federal government worker with a bachelors degree who immigrated from Ireland. Ignore any other features in your model. You must build your own Naive Bayes Classifier -- you may not use a package.

```{R Census data}

# get the data
cd <- read.csv("census.csv", stringsAsFactors = FALSE, header = TRUE)
str(cd)
```

Looking around the dataset with head(), str(), etc, shows that missing values are denoted with a ?. There is nothing inherently problematic with this, but R has special features for dealing with NA, so we will convert to that. I notice there is also a space in front of the ? sometimes.

```{R Clean census data}

# get rid of the ?
cd[cd == "?"] <- NA
cd[cd == " ?"] <- NA
```

The most obvious answer to dealing with missing data is to remove the rows. This isn't always feasible, however, depending on the size of the set and the amount of rows missing. Let's see if we can get away with the purist move or if we need to try to impute.

```{R What to do with missing values?}

# let's see what happens if we remove all with NAs using the complete cases function
c.cd <- cd[complete.cases(cd), ]

# get rid of all these spaces everywhere for no reason using the most complicated
c.cd <- as.data.frame(apply(c.cd, 2, function(y) as.character(gsub(" ", "", y))))
nrow(cd)
nrow(c.cd)
nrow(cd) - nrow(c.cd)
```
 
This is a pretty big dataset; even after purging all but the perfectly complete rows, we are looking at staying over 30k. I am going to make the judgment call that this will be enough, and worth the price of having "pure" data. It's less than 8% of a huge set.

In terms of making NB classifier, I'll start with a prototype. I'll predict the same feature, but only try one variable to get started.

```{R test model}
###################################################################################################
#  This is a test run to test the probability that a person makes 50k+ give that they are white.  #
#                                                                                                 #
#         The formula is:                                                                         #
#                                                                                                 #
#                     p.50k.white = ( (p.white.50k)(p.50k) / (p.white) )                          #
#                                                                                                 #
###################################################################################################

# number of cases
cases <- nrow(c.cd)

# extract white entries
white <- length(which(c.cd$race == "White"))

# pure probability somone is white give nothing else
p.white <- white/cases

# get 50k+ entries 
fiftyK <- length(which(c.cd$X50K == ">50K"))

# probability somone makes > 50k given nothing else
p.fiftyK <- fiftyK/cases

# get entries that are white and make over 50k
white.50 <- length(which(c.cd$race == "White" & c.cd$X50K == ">50K"))

# probability somone is white and makes more than 50k given nothing else
p.white.50 <- white.50/(length(which(c.cd$X50K == ">50K")))

# define a function to do the calculation
nb.class <- function( whiteGivenFifty, fiftyPlus, white )
{
  # execute formula 
  as.numeric(prediction <- ( (whiteGivenFifty)*(fiftyPlus) / (white) ))
  
  return(prediction)
}

# test the function 
answer <- nb.class( whiteGivenFifty = p.white.50, fiftyPlus = p.fiftyK, white = p.white )
answer
```

This makes sense intuitively. The chance of anyone randomly making 50K+ is around .24. This is very close to that, with perhaps a slight bump for being white. Note the last part was not a truly Naive Bayes. This will be slightly different given the naive independence assumptions.

The format will be a function that takes seven arguments. What to predict, the dataset with which to predict it, and the five "givens" specified in the problem. It will predict the first column, given the next five.

```{R expanded model}



# get the data from the
d <- data.frame(c.cd$X50K, c.cd$sex, c.cd$race, c.cd$workclass, c.cd$education, c.cd$native.country)

# define function as ~predict x given 5 thing~
x.given.5 <- function(pred, given1, given2, given3, given4, given5, dataset)
{
  
  # number of times the prediction occurs naturally 
  p.pred <- length(which(dataset[1] == pred))
  
  # make sure we're getting a dataset
  #d <- as.data.frame(dataset)
  cases <- nrow(dataset)
    
  # we will have it predict the first column given the next 5
  # note that "given1" is [2] because the target is in [1]
  
  # get the numbers where are givens are true and not true
  # this translates to "The probabilty that subect is white and makes >50"
  # and the probability that they are make 50K given they're not white
  g1 <- length(which(dataset[2] == given1 & dataset[1] == ">50K" ))
  x.g1 <- length(which(dataset[2] != given1 & dataset[1] == ">50K"))

  
  # find their probability 
  p.g1 <- g1/cases
  px.g1 <- x.g1/cases

  # repeat this pattern for the ret of the givens.
  # given more time, one might write functions to smooth this slightly
  g2 <- length(which(dataset[3] == given2 & dataset[1] == ">50K"))
  x.g2 <- length(which(dataset[3] != given2 & dataset[1] == ">50K"))
  
  p.g2 <- g2/cases
  px.g2 <- x.g2/cases  
  
  # for the third...
  g3 <- length(which(dataset[4] == given3 & dataset[1] == ">50K" ))
  x.g3 <- length(which(dataset[4] != given3 & dataset[1] == ">50K"))
  
  p.g3 <- g3/cases
  px.g3 <- x.g3/cases  

  # for the fouth...
  g4 <- length(which(dataset[5] == given4 & dataset[1] == ">50K" ))
  x.g4 <- length(which(dataset[5] != given4 & dataset[1] == ">50K"))
  
  p.g4 <- g4/cases
  px.g4 <- x.g4/cases
  
  # for the final
  g5 <- length(which(dataset[6] == given5 & dataset[1] == ">50K" ))
  x.g5 <- length(which(dataset[6] != given5 & dataset[1] == ">50K"))
  
  p.g5 <- g5/cases
  px.g5 <- x.g5/cases

  # execute the niave bayes formula 
  numerator <- p.g1 * p.g2 * p.g3 * p.g4 * p.g5 * p.pred
  denominator <- px.g1 * px.g2 *  px.g3 *  px.g4 *  px.g5

  answer <- numerator/(numerator + denominator)
  
  return(answer) 
}

# call the formula 
result <- x.given.5(pred = ">50K", given1 = "Male", given2 = "White", given3 = "Federal-gov",
               given4 = "Bachelors", given5 = "Ireland", dataset = d)

control <- pure.prob50 <- length(which(c.cd$X50K == ">50K"))/cases

print("Given our factors:")
result
print("Pure Probability:")
control
```

This makes sense in the light of the last model with race only: working a steady government job, being male, and having a college education further bump up the odds of someone breaking the 50k mark.

(20 pts) Perform 10-fold cross validation on your algorithm to tune it and report the final accuracy results.

```{R cross validate and test}

# caret provides a cross validation funciton
library(caret)

# create a partition to divide the data
nb.partition <- createDataPartition(y = d$c.cd.X50K, p = 0.50, list = FALSE)

# create the sets 
nb.train <- d[nb.partition, ]
nb.test <- d[-nb.partition, ]

# create the column for the prediction
nb.test$Pred <- 0

# apply the fucntion to the data frame - this isn't excactly the most idiomatic R
# code I've ever written, but it works fine for this applications as I wanted to 
# be very explicit in testing a prototype function 

for ( i in 1:nrow(nb.test) ) 
{
  nb.test$Pred[i] <- x.given.5(   pred = ">50K", 
                                  given1 = as.character(nb.test[i, 2]),
                                  given2 = as.character(nb.test[i, 3]), 
                                  given3 = as.character(nb.test[i, 4]),
                                  given4 = as.character(nb.test[i, 5]),
                                  given5 = as.character(nb.test[i, 6]),
                                  dataset = nb.test                  )
}
```

Keep in mind we're dealing with probabilistic data here, so we need to round it to round number/dummy codes and then we can sub in the character values we are looking for. 

```{R clean prediction data}

# create and empty row of zeros 
nb.test$Pred <- round(nb.test$Pred, digits = 0)

# set the numbers from the dummy codes
nb.test$Pred[which(nb.test$Pred == 0) ] <- ">50K"
nb.test$Pred[which(nb.test$Pred == 1) ] <- "<=50K"

# find the acccuracy
accuracy<- length(which(nb.test$Pred == nb.test$c.cd.X50K))/nrow(nb.test)
accuracy
```

We can now cross validate to make sure we aren't over fitting. We'll set aside a chunk of data to test on

```{R cross validation}

# create some data partitions 

# this is the one we will predict 
kd.partition1  <- createDataPartition(y = d$c.cd.X50K, p = 0.1, list = FALSE)

# these will become the trainers
kd.partition2  <- createDataPartition(y = d$c.cd.X50K, p = 0.1, list = FALSE)
kd.partition3  <- createDataPartition(y = d$c.cd.X50K, p = 0.1, list = FALSE)
kd.partition4  <- createDataPartition(y = d$c.cd.X50K, p = 0.1, list = FALSE)
kd.partition5  <- createDataPartition(y = d$c.cd.X50K, p = 0.1, list = FALSE)
kd.partition6  <- createDataPartition(y = d$c.cd.X50K, p = 0.1, list = FALSE)
kd.partition7  <- createDataPartition(y = d$c.cd.X50K, p = 0.1, list = FALSE)
kd.partition8  <- createDataPartition(y = d$c.cd.X50K, p = 0.1, list = FALSE)
kd.partition9  <- createDataPartition(y = d$c.cd.X50K, p = 0.1, list = FALSE)
kd.partition10 <- createDataPartition(y = d$c.cd.X50K, p = 0.1, list = FALSE)


# combine to save time
kd.train.partition <- kd.partition2 + kd.partition3 + kd.partition4 + kd.partition5 + kd.partition6 +
                                      kd.partition7 + kd.partition8 + kd.partition9 + kd.partition10

# create data sets
kd.train <- d[kd.train.partition, ]
kd.test <- d[kd.partition1, ]


# test one out
result <- x.given.5(pred = ">50K", given1 = "Male", given2 = "White", given3 = "Federal-gov",
               given4 = "Bachelors", given5 = "Ireland", dataset = kd.train)
result
 
# create a column
kd.test$Pred <- 0

# apply the model
for ( i in nrow(kd.test) ) 
{
  kd.test$Pred[i] <-   x.given.5(   pred = ">50K", 
                                  given1 = as.character(kd.test[i, 2]),
                                  given2 = as.character(kd.test[i, 3]), 
                                  given3 = as.character(kd.test[i, 4]),
                                  given4 = as.character(kd.test[i, 5]),
                                  given5 = as.character(kd.test[i, 6]),
                                  dataset = kd.test                  )
}

# create and empty row of zeros 
kd.test$Pred <- round(kd.test$Pred, digits = 0)

# set the numbers from the dummy codes
kd.test$Pred[which(kd.test$Pred == 0) ] <- ">50K"
kd.test$Pred[which(kd.test$Pred == 1) ] <- "<=50K"

# find the acccuracy
k.accuracy<- length(which(kd.test$Pred == kd.test$c.cd.X50K))/nrow(kd.test)
k.accuracy
```

I do not know why, but calling the function on the partitioned data reduces the accuracy from 72% to 24% (Which is the number you get if you guess 'yes' for every case in the dataset). What is supposed to be happening is that we break the set in to ten parts taking random entries from the raw data. We'd create a training set from some and a test from the other, thus making what would be proportionally similar but distinct sets. This allows us to tune the model in such a way as to avoid over fitting.

# Question 2

This question presents and case study scenario about finding if a buyer was wronged in a housing purchase 

```{R get housing data}

# get the data and store in "rhd" (raw housing data), inspect
rhd <- read.csv("uffidata.csv", header = TRUE, stringsAsFactors = FALSE)
str(rhd)
```

(10 pts) Are there outliers in the data set? If so, what is the appropriate action and how are they discovered?

There are a few things that need to be fixed first. I didn't see any missing data, but one thing jumps out right away: the sales price is stored as a text data, complete with a comma and an extra space doing nothing on the end (may god have mercy on their soul). This prevents us from converting to a number so we need to get rid of at least the commas before we can use them.


```{R clean housing data}

# I like to use a copy for messing around with in case I need to see the original
hd <- rhd

# in keeping with this weeks theme, there are a bunch of spaces we don't need
hd <- as.data.frame(apply(hd, 2, function(y) as.character(gsub(" ", "", y))))

# we also need to get the commas 
hd <- as.data.frame(apply(hd, 2, function(y) as.character(gsub(",", "", y))))

# now we can try to get something done with conversion
# it's a factor now, which means it will mess up the numbers
hd$Sale.Price <- as.character(hd$Sale.Price)

# now for numerics
hd$Sale.Price <- as.numeric(hd$Sale.Price)

str(hd)
```

This is looking a lot more civilized. Let's search for outliers...we'll start with guideline of 3 standard deviations from the mean (score of 3). We will then consider whether this makes sense in the context of the size and shape of the data.

```{R outliers?}

# any missing info?
sum(is.na(hd))

# for some insane reason this number is stored as a character
hd$Sale.Price <- as.numeric(hd$Sale.Price)

# get the requirments for a zscore 

# get the mean 
mean.price <- mean(hd$Sale.Price)

# get the standard deviations 
sd.price <- sd(hd$Sale.Price)

# do the calculations and store the results 
hd$Z.Sale <- abs(((hd$Sale.Price - mean.price)/sd.price))

print("Z = 3.0")
which(hd$Z.Sale > 3.0)

print("Z = 1.5")
which(hd$Z.Sale > 1.5)
```
To answer the question posed: in short yes, we've got some outliers. Which ones count will require a bit more reasoning. We started with the common cutoff of 3.0 and tried 1.5 next to be diligent. Keeping in mind this is a much smaller dataset than was previously used, I will probably go with a Z score from here on out. The reasoning behind this is that using 1.5 results in 9 outliers in a 99 entry data set. That is a shade over 9% of entries. Almost 1 in 10 isn't an outlier, it's more like the upper quartile of your data. Using 3, however, tags about 3% of the data as on outlier, which seems much more sensible. 

While I am pretty confident in this decision, there are a few things we can do to get more information before locking into something. 

```{R Outliers pt. 2}

# let's look at the actual records vs mean as well as the min/max

# find outliers using 3.0
outliers <- hd[which(hd$Z.Sale > 3.0), ]

print("min, mean, max:")
min(hd$Sale.Price)
mean.price
max(hd$Sale.Price)
print("Outliers:")
outliers$Sale.Price
```

This is just to provide a general sense of what is going on with the data in regards to the possible outliers. We don't see an outliers in the low end and they do seem pretty far away from the rest of the data. A visual inspection will round out the analysis.

(5 pts) Using visual analysis of the sales price with a histogram, is the data normally distributed and thus amenable to parametric statistical analysis? What are the correlations to the response variable and are there collinearities?

(2 pts) Is the presence or absence of UFFI alone enough to predict the value of a residential property?
(Note: The most efficient workflow doesn't support answering these in perfect order, but we will get to them all)

This allows us to get a little more perspective. For example, before, we knew there 3 outliers, but this helps show the degree to which they are outliers. If the three numbers were 250000, 250000, and 1,000,000, it would be very clear we should ditch the highest one. This spread is a little more tame. A histogram might be of some use. Without removing the outliers, the data is not parametric. There some vague bell shape to it, but the outliers stretch it such that any parametric methods cannot be trusted. 

```{R Outliers pt.3}

hist(hd$Sale.Price)
```

Given this histogram and the fact that were no low outliers, I am going to stick with 3 Z score. In terms of the how to handle them, I will remove them; the 300k entry is a textbook outlier, even causing a break in this histogram, and the others extend it as well. There are other entries to base a model on that are almost that high but still sit in the curve, so I think it is best. Let's get ride of the outliers are make a new graph.

```{R remove outliers}

hd <- hd[-c(97,98,99), ]
hist(hd$Sale.Price)
```

This is quite parametric. Before we move on though, the data need to be converted to appropriate types. R doesn't like it when I got right from a factor to a number, so this is a little roundabout. 

```{R transform data types}

# get the caret library 
library(caret)

# remove irrelevant columns
hd$Observation <- NULL
hd$Z.Sale <- NULL

# set this back to a number 
hd$Sale.Price <- as.numeric(hd$Sale.Price)

hd$Year.Sold <- as.character(hd$Year.Sold)
hd$Year.Sold <- as.numeric(hd$Year.Sold)

hd$Bsmnt.Fin_SF <- as.character(hd$Bsmnt.Fin_SF)
hd$Bsmnt.Fin_SF <- as.numeric(hd$Bsmnt.Fin_SF)

hd$Enc.Pk.Spaces <- as.character(hd$Enc.Pk.Spaces)
hd$Enc.Pk.Spaces <- as.numeric(hd$Enc.Pk.Spaces)

hd$Lot.Area <- as.character(hd$Lot.Area)
hd$Lot.Area <- as.integer(hd$Lot.Area)

hd$Living.Area_SF <- as.character(hd$Living.Area_SF)
hd$Living.Area_SF <- as.numeric(hd$Living.Area_SF)
```

We can now try a few different methods for finding correlations. While somewhat clumsy, we can get what we need by calling the cor function on all the variables...

```{R correlations}

# the explicit way...
print("Year.Sold")
cor(hd$Sale.Price, as.numeric(hd$Year.Sold))
print("UFFI.IN")
cor(hd$Sale.Price, as.numeric(hd$UFFI.IN))
print("Brick.Ext")
cor(hd$Sale.Price, as.numeric(hd$Brick.Ext))
print("X45.Yrs.")
cor(hd$Sale.Price, as.numeric(hd$X45.Yrs.))
print("Bsmnt.Fin_SF")
cor(hd$Sale.Price, as.numeric(hd$Bsmnt.Fin_SF))
print("Lot.Area")
cor(hd$Sale.Price, as.numeric(hd$Lot.Area))
print("Enc.Pk.Spaces")
cor(hd$Sale.Price, as.numeric(hd$Enc.Pk.Spaces))
print("Living.Area_SF")
cor(hd$Sale.Price, as.numeric(hd$Living.Area_SF))
print("Pool")
cor(hd$Sale.Price, as.numeric(hd$Pool))
```

...but there are also visual methods.

```{R correlations 2}

# get psych library for pairs.panels
library(psych)

#cor(as.numeric(hd$UFFI.IN), hd$Sale.Price)
pairs.panels(hd, gap = 0, cex.labels = 1)
```

(2 pts) Is the presence or absence of UFFI alone enough to predict the value of a residential property?

This graph shows there are several features that have a large impact on price, so this is a resounding no. What is relevant to our case study scenario, however, is that is IS one of the traits with an impact. It has a correlation of -0.2 indicating a small but real dip.

(4 pts) Is UFFI a significant predictor variable of selling price when taken with the full set of variables available? Yes (see below)
(5 pts) On average, how do we expect UFFI will change the value of a property? Decrease (See below)

We will get more specific and quantitative about what this means as we progress, but for now I am considering it certainly worthy of investigation because it is a non-negligible correlation and apparently more impactful than being more than 45 years old, having a finished basement, and only negligibly less than having a pool.  

~~~

(15 pts) What is the ideal multiple regression model for predicting home prices in this data set? Provide a detailed analysis of the model, including Adjusted R-Squared, RMSE, and p-values of principal components. Use backfitting to build the model. - We will see these in the model outputs, and will add RMSE for good measure later on and to address this part of the question. 

Before we can go about making any useful inferences from this dataset, we need to split it up and randomize it; the table is sorted by price, our variable of interest, which will likely undermine our models. 

```{R Housing model - training and validation}

# create a partition
hd.partition <- createDataPartition(y = hd$Sale.Price, p = 0.50, list = FALSE)

# split the data with it
hd.train <- hd[hd.partition, ]
hd.test <- hd[-hd.partition, ]
```

Here we will make our first run at a backfit linear regression model. I will run it like this, remove the factor with the lowest p value, and run it again. I'll continue this process until only the significant values remain. Rather than running nearly identical cells of code, I'll show the final model having described the process I used to get it. 

```{R Model with housing data (initial)}

#hd$Sale.Price <- factor(hd$Sale.Price) 

hm <- lm(Sale.Price ~ Year.Sold + UFFI.IN + Brick.Ext + X45.Yrs. +Bsmnt.Fin_SF  + 
                          Lot.Area + Enc.Pk.Spaces + Living.Area_SF + Central.Air + Pool, data = hd.train)

summary(hm)
```

Here is the lean, pvalue significant-only version of the model. For experimentation, we will include the variable of interest in another model. 

```{R Model with housing data, refined}

# create out models
hm <- lm(Sale.Price ~ Year.Sold + Bsmnt.Fin_SF + Enc.Pk.Spaces + Living.Area_SF, data = hd.train)
hm.u <- lm(Sale.Price ~ Year.Sold + Bsmnt.Fin_SF + Enc.Pk.Spaces + Living.Area_SF + UFFI.IN, data = hd.train)

summary(hm)
summary(hm.u)
```

There are few keys things to take from this. Firstly, the elephant in the room, is that our variable in question did not make the cut. I am not going to write the investigation off based on one model, but it is certainly worth noting. 

On the other hand, the model where we put the variable in question back had a slightly better accuracy by MRS. Given the earlier analysis suggested a correlation and that pvalue isn't;t the only metric, we are going to dig a little deeper.

Let's do a few tests.

```{R test house prediction}

# make a prediction 
hd.test$Pred <- predict(hm, hd.test)

# find the error 
hd.test$PredErr <- abs(hd.test$Sale.Price - hd.test$Pred)

# I'll show the range for perspective
range(hd.test$Sale.Price)

# average error amd RMSE
mean(hd.test$PredErr)
RMSE(hd.test$Sale.Price, hd.test$Pred)
```

Let's see what happens if we use the UFFI.

```{R test house prediction, UFFI}

# make a prediction 
hd.test$Pred.u <- predict(hm.u, hd.test)

# find the error 
hd.test$PredErr.u <- abs(hd.test$Sale.Price - hd.test$Pred.u)

# I'll show the range for perspective
range(hd.test$Sale.Price)

# average error amd RMSE
mean(hd.test$PredErr.u)
RMSE(hd.test$Sale.Price, hd.test$Pred.u)
```

By the other metric, the model with the UFFI is slightly better. 

```{R test house prediction rpart}

library(rpart)

# model using rpart 
hm.r <- rpart(Sale.Price ~ Year.Sold + X45.Yrs. + Enc.Pk.Spaces + Living.Area_SF, data = hd.train)

# examine the model 
summary(hm.r)

# make a prediction 
hd.test$Pred.r <- predict(hm.r, hd.test)

# find the error 
hd.test$PredErr.r <- abs(hd.test$Sale.Price - hd.test$Pred.r)

# I'll show the range for perspective
range(hd.test$Sale.Price)

# average error amd RMSE
mean(hd.test$PredErr.r)
RMSE(hd.test$Sale.Price, hd.test$Pred.r)
```

In general this model fairs worse, but in the name of diligence, let' stick with the pattern of testing with and without the UFFI.

```{R Test house prediction, rpart, uffi}

hm.ru <- rpart(Sale.Price ~ Year.Sold + X45.Yrs. + UFFI.IN + Enc.Pk.Spaces + Living.Area_SF, data = hd.train)

summary(hm.ru)

# make a prediction 
hd.test$Pred.ru <- predict(hm.ru, hd.test)

# find the error 
hd.test$PredErr.ru <- abs(hd.test$Sale.Price - hd.test$Pred.ru)

# I'll show the range for perspective
range(hd.test$Sale.Price)

# average error amd RMSE
mean(hd.test$PredErr.ru)
RMSE(hd.test$Sale.Price, hd.test$Pred.ru)
```

The part model isn't as effective with our data. Moving on!

The fact that pvalue ruled out the variable of interest BUT including it tells me that isn't the right criteria for this context. 
Let's try backfitting again, but this time rule out using a different metric. The standard error is related the metric that suggested there might be more significance to the UFFI metric. I'll rule things out with that this time 

```{R Another method }

# create a different model 
hm.se <- lm(Sale.Price ~ Year.Sold + UFFI.IN + Brick.Ext + X45.Yrs. + Bsmnt.Fin_SF + 
                          Lot.Area + Enc.Pk.Spaces + Living.Area_SF + Central.Air + Pool, data = hd.train)

# make a prediction 
hd.test$Pred.se <- predict(hm.se, hd.test)

# find the error 
hd.test$PredErr.se <- abs(hd.test$Sale.Price - hd.test$Pred.se)

# I'll show the range for perspective
range(hd.test$Sale.Price)

# check out the model 
summary(hm.se)

# average error amd RMSE
mean(hd.test$PredErr.se)
RMSE(hd.test$Sale.Price, hd.test$Pred.se)
```

This method also rules out the UFFI, so we will have to continue and seek some more data from other methods. 

(5 pts) If the home in question is older than 45 years old, doesn't have a finished basement, has a lot area of 5000 square feet, has a brick exterior, 2 enclosed parking spaces, 1700 square feet of living space, central air, and no pool, what is its predicted value and what are the 95% confidence intervals of this home with UFFI and without UFFI?

It's worth noting that the criteria presented here don't perfectly reflect what comes up in the linear model, so I can't quite apply them directly. We can, however, look for trends. 

I'll add the new homes for testing to the bottom of the test set

```{R calculate the home value with 95% CI}

# we will add the rows with new data to make a prediction
nd <- c( Year.Sold = 2016 , Sale.Price = 0, UFFI.IN = 0, Brick.Ext = 1,X45.Yrs. = 1, Bsmnt.Fin_SF = 0, Lot.Area = 50000, 
         Enc.Pk.Spaces = 2, Living.Area_SF = 1700, Central.Air = 1, Pool = 0, Pred = 0, PredErr = 0, Pred.u = 0, PredErr.u = 0,
         Pred.r = 0, PredErr.r = 0, Pred.ru = 0, PredErr.ru = 0 )


nd2 <- c( Year.Sold = 2016 , Sale.Price = 0, UFFI.IN = 1, Brick.Ext = 1, X45.Yrs. = 1, Bsmnt.Fin_SF = 0, Lot.Area = 50000,
          Enc.Pk.Spaces = 2, Living.Area_SF = 1700, Central.Air = 1, Pool = 0, Pred = 0, PredErr = 0, Pred.u = 0, PredErr.u = 0,
          Pred.r = 0, PredErr.r = 0, Pred.ru = 0, PredErr.ru = 0) 

# make the attachment 
hd.test <- rbind(hd.test, nd)
hd.test <- rbind(hd.test, nd2)

# create a new model 
ndm <- lm(Sale.Price ~ Year.Sold + X45.Yrs. + Bsmnt.Fin_SF + Lot.Area + UFFI.IN + Brick.Ext + Enc.Pk.Spaces + Living.Area_SF
          + Central.Air + Pool, data = hd.train)

# see how we did 
hd.test$Pred.nd <- predict(ndm, hd.test)
tail(hd.test, 2)
```

This does show loss due to the presence of UFFI. I couldn't recreate this in any of my models. But they did suggest there was more to the story. The model has a MRSE of about 17000. 

Perhaps the confidence interval will help us gain some clarity.

The error is calculated using the value of the Z score associated with 95% confidence in a normal dataset and the standard error of the model.

```{R Error}

# error
err <- (1.960 * 16430)

# prices
price.no.u <- 315761.9
prince.w.u <- 296614.8

# low, high for each
ci.wu.l <- price.no.u - err
ci.wu.h <- price.no.u + err

ci.nu.l <- prince.w.u - err
ci.nu.h <- prince.w.u + err

# display results 
ci.wu.l
ci.wu.h

ci.nu.l
ci.nu.h
```

This shows a range of around 60,000 dollars. That is not at all precise or reliable. 

The model obtained using the features in the question offered a seemingly concrete answer: the presence of UFFI would have cost a measurable amount of money, and they should be compensated proportionally. However, the 95% confidence interval was quite wide, suggesting it not be used for quantification.

If I had to make a decision:

There IS a correlation between decrease in value and UFFI. It is not significant in linear regression models, but the accuracy of the models was only in the 70-80s, and the discrepancy could have been accounted for in that. The correlation and the model with the poor 95% confidence interval suggest compensation is owed, and I would use the MRSE of the linear model I created as the amount.

(2 pts) If $215,000 was paid for this home, by how much, if any, did the client overpay, and how much compensation is justified due to over payment?

I think these models are all over-predicting: They would have called 215,000 as steal, UFFI or no.

# Question 3

The titanic dataset is a classic practice problem for data work. It is often approached with random forests, but an also be addressed with linear regression, which is what we will do in this analysis.


```{R QUESTION 3 - Titanic}

# get titanic data 
td <- read.csv("titanic_data.csv")
str(td)
```

(5 pts) Divide the provided Titanic Survival Data into two subsets: a training data set and a test data set. Use whatever strategy you believe it best. Justify your answer.

We will still start with a simple split and see how it works, if the proportions are off, we can use a more advanced split, like from the caret package.

```{R Train and Validate}

# divide the data into train/test
td.test <- td[1:445, ]
td.train <- td[446:890, ]

# See if the survivors are distributed fairly 
prop.table(table(td.train$Survived))
prop.table(table(td.test$Survived))
```

You can also use caret partitioning:

```{R Train and Validate2}

# create a partition
td.partition <- createDataPartition(y = td$Survived, p = 0.50, list = FALSE)

# split the data with it
td.trainCaret <- td[td.partition, ]
td.testCaret <- td[-td.partition, ]

# observe proportions 
prop.table(table(td.trainCaret$Survived))
prop.table(table(td.testCaret$Survived))
```

I am choosing to use the simple split for this question. In terms of justifying my answer, I ran the analysis with each method and the results were very close, with a slight advantage to the simple split. I also looked at the prop tables and the distributions, at least in terms of survival, were very similar. Given that one is easier, had similar proportions, and a slightly better result, I am proceeding with it. Note that this might not be the case in all datasets and applying simply splits to some datasets, like ones that have been sorted based on some criteria you are not aware of can completely undermine your model. Looking through the data first and perhaps trying different methods are considered due diligence for data work in many cases.

(15 pts) Construct a logistic regression model to predict the probability of a passenger surviving the Titanic accident. Test the statistical significance of all parameters and eliminate those that have a p-value < 0.05.

```{R simple model}

# create a linear model
m <- lm(Survived ~ PassengerId + Pclass + Sex + Age + SibSp + Parch +
          Fare + Embarked, data = td.train)

# apply it
pred <- predict(m, data = td.test)

# round to binomial answers
td.test$Pred <- round(predict(m, td.test), digits = 0)

summary(m)
length(which(td.test$Pred == td.test$Survived))/nrow(td.test)
```

Using all the data produced a pretty bad result. A lot of tutorials on this dataset start by simply guessing that everyone dies or that all the men die with no fine modeling whatsoever, and this preforms about as well a they do. We can look at the summary though, to find where some key patterns emerged, and we will use them to build something a little more refined. After looking through the summary, I have selected the p-value significant factors. Cutting out the others reduced some of the noise and offers some improvement.

```{R simple model step 2}

# create a model using fewer variables
m <- lm(Survived ~ Pclass + Sex + Age + SibSp, data = td.train)

# apply the model
pred <- predict(m, data = td.test)

# make a prediction
td.test$Pred <- round(predict(m, td.test), digits = 0)

# see accuracy 
length(which(td.test$Pred == td.test$Survived))/nrow(td.test)
```

While the input was considerably more refined, the gain was fairly small, so I will try a new model we used on some recent homework. It can be found in the rpart library. This allows us to use regression trees.

```{R rpart }

library(rpart)

# let's try the same thing again with this model
m <- rpart(Survived ~ Pclass + Sex + Age + SibSp, data = td.train)

# make a prediciton
pred <- predict(m, data = td.test)

# we're rounding because this is binomal
td.test$Pred <- round(predict(m, td.test), digits = 0)

# see comparison
length(which(td.test$Pred == td.test$Survived))/nrow(td.test)
```

This method seems to offer a considerable boost, so we will stick with it. It's a regression tree model, so I will also use the lm to do more typical logistic regression as specified in the homework I just couldn't help experiment.

One aspect of data science that is often explored with this data set is feature engineering. This is the process of extrapolating new columns from ones that are already there. This is often said to be where the art data meets the science, and even small amounts of thoughtful feature engineering can offer a considerable boost. We've seen that the Pclass feature is of considerable importance, and this gives us a clue as to where to look for more hidden features. In the titanic data, the title of the passenger is embedded in their name. With titles and military ranks almost certainly tied to socioeconomic status, they might be useful to our model.

It is important to note than we need to engineer the validation dataset in the same way we engineer the test set (the model will not run if it sees there is a Title column in one and not the other). Rather than use a bunch of "boilerplate" code, it is common practice to "Stack" the two sets on top of one another and then split then back up. 

note: the inspiration behind this cell as well of a few lines of the code are taken/modified from a handful of titanic data science lessons.

(10 pts) Test the model against the test data set and determine its prediction accuracy (as a percentage correct).

```{R Title feature engineering }

# remove the old predicitons
td.test$Pred <- NULL
td.test$Pred2 <- NULL

# stack the data
merged <- rbind(td.train, td.test)

#merged <- as.data.frame(apply(merged, 2, function(y) as.character(gsub(" ", "", y))))

# change the namesto characters 
merged$Name <- as.character(merged$Name)

# split the names on spaces, periods, commas, etc, acces them by index, stores them in a new column
merged$Title <- sapply(merged$Name, FUN = function(x) {strsplit(x, split='[,.]')[[1]][2]})


# refactor the column
merged$Title <- factor(merged$Title)

# split them back up
td.test <- merged[1:445, ]
td.train <- merged[446:890, ]

# let's try the same thing again 
m <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Title, data = td.train)

pred <- predict(m, data = td.test)

td.test$Pred <- round(predict(m, td.test), digits = 0)

print("Rpart Accuracy:")
length(which(td.test$Pred == td.test$Survived))/nrow(td.test)
```

(2 pts) State the model as a regression equation.

Coefficients:
 (Intercept)        Pclass       Sexmale           Age         SibSp      Title Dr  Title Master  
    1.403290     -0.144703     -1.095895     -0.004067     -0.113119      0.173237      0.887614  
  Title Miss     Title Mme      Title Mr     Title Mrs      Title Ms     Title Rev  
   -0.216547     -0.160972      0.343970     -0.094576            NA      0.219015 
   
   1.403290 - Pclass(0.144703) - Sexmale(1.095895 ) -Age(0.004067) - SibSp(0.113119) + Title Dr(0.173237)
   + Title Master(0.887614) - Miss(0.216547) + Mr(0.343970) - Mrs(0.094576) + Rev(0.219015)
 
 
# Question 4

(10 pts) Elaborate on the use of kNN and Naive Bayes for data imputation. Explain in reasonable detail how you would use these algorithms to impute missing data and why it can work.

The heart of the issue in using these two algorithms for imputation is how the complement each other.

Imputation of data simply means the prediction of missing values (as opposed to discarding them). The approach needs to be adjusted to what you are trying to predict. Though the are both classifiers, the two algorithm are complementary in this way because k-NN uses numerical data and NB uses probabilities derived from categorical data. 

For instance, we have used k-NN to predict tumor types based on numerical size data, and done similar procedures on the famous iris dataset. Because it is based on euclidean distance, it handles number that have range and decimal values. It "draws" them on a graph, precisely, and measures the distance. This will help impute data the has NUMERICAL INPUTS and CATAGORICAL OUTCOMES. 

NB is probabilistic, and the probabilities are derived from counts yes/no counts in existing data. For instance, counting the number of times a patient was a smoker or a nonsmoker. These become decimal figures eventually, but only in the context of probability. The answer is often rounded to the nearest whole number to give a classification. When used this way it takes CATAGORICAL INPUT and gives CATAGORICAL OUTCOMES. 

To apply these to recent work, I would used the NB to predict if someone was male or female in context of the titanic. Did the live? What was their class on a scale of 1-3, etc.

I would use k-NN to predict if a house had a basement or not, for example. Number of dollars in price, number of square footage, etc.
 